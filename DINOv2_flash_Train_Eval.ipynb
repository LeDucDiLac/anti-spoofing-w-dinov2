{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn, optim\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Define datasets and models\n",
        "class FeatureDataset(Dataset):\n",
        "    def __init__(self, features, labels, paths):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.paths = paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx], self.paths[idx]\n",
        "\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, layers, input_dim, output_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        prev_dim = input_dim\n",
        "        for layer_dim in layers:\n",
        "            self.layers.append(nn.Linear(prev_dim, layer_dim))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            prev_dim = layer_dim\n",
        "        self.layers.append(nn.Linear(prev_dim, output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "train_features_path = \"/content/drive/MyDrive/antispoofing/features-LCC_train-dinov2_vitb14.npy\"\n",
        "train_labels_path = \"/content/drive/MyDrive/antispoofing/labels-LCC_train-dinov2_vitb14.npy\"\n",
        "train_paths_path = \"/content/drive/MyDrive/antispoofing/paths-LCC_train-dinov2_vitb14.npy\"\n",
        "\n",
        "dev_features_path = \"/content/drive/MyDrive/antispoofing/features-LCC_dev-dinov2_vitb14.npy\"\n",
        "dev_labels_path = \"/content/drive/MyDrive/antispoofing/labels-LCC_dev-dinov2_vitb14.npy\"\n",
        "dev_paths_path = \"/content/drive/MyDrive/antispoofing/paths-LCC_dev-dinov2_vitb14.npy\"\n",
        "\n",
        "train_features = np.load(train_features_path)\n",
        "train_labels = np.load(train_labels_path)\n",
        "train_paths = np.load(train_paths_path, allow_pickle=True)\n",
        "\n",
        "dev_features = np.load(dev_features_path)\n",
        "dev_labels = np.load(dev_labels_path)\n",
        "dev_paths = np.load(dev_paths_path, allow_pickle=True)\n",
        "\n",
        "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "dev_features = torch.tensor(dev_features, dtype=torch.float32)\n",
        "dev_labels = torch.tensor(dev_labels, dtype=torch.long)\n",
        "\n",
        "train_dataset = FeatureDataset(train_features, train_labels, train_paths)\n",
        "dev_dataset = FeatureDataset(dev_features, dev_labels, dev_paths)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Model configurations\n",
        "input_dim = 768\n",
        "output_dim = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzwvjfeXGYvv",
        "outputId": "645a5b39-fabe-4448-84aa-68051016c9aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nn_configurations = {\n",
        "    \"2-layer NN\": [128, 64],\n",
        "    \"3-layer NN\": [256, 128, 64],\n",
        "    \"4-layer NN\": [512, 256, 128, 64],\n",
        "    \"5-layer NN\": [512, 256, 128, 64, 32],\n",
        "}\n",
        "\n",
        "# Training and evaluation\n",
        "results = {}\n",
        "\n",
        "for name, layers in nn_configurations.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    model = SimpleNN(layers, input_dim, output_dim).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Training\n",
        "    epochs = 100\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_features, batch_labels, _ in train_loader:\n",
        "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    misclassified_paths = []\n",
        "    with torch.no_grad():\n",
        "        for batch_features, batch_labels, batch_paths in dev_loader:\n",
        "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "            outputs = model(batch_features)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            all_labels.extend(batch_labels.cpu().numpy())\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            for i, (label, pred, path) in enumerate(zip(batch_labels.cpu().numpy(), predictions.cpu().numpy(), batch_paths)):\n",
        "                if label != pred:\n",
        "                    misclassified_paths.append((path, label, pred))\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_predictions) * 100\n",
        "    results[name] = {\"accuracy\": accuracy, \"misclassified_paths\": misclassified_paths}\n",
        "    print(f\"{name} Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Plot accuracy comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(results.keys(), [result[\"accuracy\"] for result in results.values()])\n",
        "plt.title(\"Accuracy Comparison Across Models\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.grid(axis=\"y\")\n",
        "plt.show()\n",
        "\n",
        "# Display confusion matrices\n",
        "for name, result in results.items():\n",
        "    print(f\"Confusion Matrix for {name}\")\n",
        "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[0, 1])\n",
        "    disp.plot(cmap=\"viridis\", values_format=\"d\")\n",
        "    plt.title(f\"Confusion Matrix - {name}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "esj6KrpmSJ2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ImprovedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.fc4 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "name = \"3 - Layer Improved Model\"\n",
        "print(f\"Training {name}...\")\n",
        "model = ImprovedNN(input_dim, output_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "# Training\n",
        "epochs = 300\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_features, batch_labels, _ in train_loader:\n",
        "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_features)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "misclassified_paths = []\n",
        "with torch.no_grad():\n",
        "    for batch_features, batch_labels, batch_paths in dev_loader:\n",
        "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "        outputs = model(batch_features)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        all_labels.extend(batch_labels.cpu().numpy())\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        for i, (label, pred, path) in enumerate(zip(batch_labels.cpu().numpy(), predictions.cpu().numpy(), batch_paths)):\n",
        "            if label != pred:\n",
        "                misclassified_paths.append((path, label, pred))\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_predictions) * 100\n",
        "results[name] = {\"accuracy\": accuracy, \"misclassified_paths\": misclassified_paths}\n",
        "print(f\"{name} Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKH76g-eMwIa",
        "outputId": "2ad13e53-5515-4ce9-bef4-1eb0c2ddc9e4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training improved model...\n",
            "5-layer NN Accuracy: 97.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(results.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agfMJHYKNZeP",
        "outputId": "af2e583c-19e5-4153-e692-c9eb1a676fd9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LightweightCNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LightweightCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim // 4 * 64, 128)\n",
        "        self.relu_fc1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Adding channel dimension for Conv1D\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu_fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "name = \"Lightweight CNN model\"\n",
        "print(f\"Training {name}...\")\n",
        "model = LightweightCNN(input_dim, output_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "# Training\n",
        "epochs = 300\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_features, batch_labels, _ in train_loader:\n",
        "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_features)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "misclassified_paths = []\n",
        "with torch.no_grad():\n",
        "    for batch_features, batch_labels, batch_paths in dev_loader:\n",
        "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "        outputs = model(batch_features)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        all_labels.extend(batch_labels.cpu().numpy())\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        for i, (label, pred, path) in enumerate(zip(batch_labels.cpu().numpy(), predictions.cpu().numpy(), batch_paths)):\n",
        "            if label != pred:\n",
        "                misclassified_paths.append((path, label, pred))\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_predictions) * 100\n",
        "results[name] = {\"accuracy\": accuracy, \"misclassified_paths\": misclassified_paths}\n",
        "print(f\"{name} Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpecQxcIOgEG",
        "outputId": "1fe405d1-95c5-43bb-a3f2-3c0430aa7315"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training improved model...\n",
            "Light Weight model Accuracy: 94.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUwv48CPTAu0",
        "outputId": "508853d9-7106-4dfa-98ab-cac9cf00bd18"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Light Weight model Accuracy: 42.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iwn3Hka5SWoy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}